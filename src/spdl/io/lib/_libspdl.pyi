# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree.
#
# pyre-ignore-all-errors
#
# @generated
# This file is generated by stubgen.py
# and should not be edited manually.
# Use spdl/io/lib/stubgen.py to generate stubs.


from collections.abc import Mapping, Sequence, Set
from typing import Annotated, overload

from numpy.typing import ArrayLike


class DemuxConfig:
    """
    Demux configuration.

    See the factory function :py:func:`~spdl.io.demux_config`.
    """

    def __init__(self, format: str | None = None, format_options: Mapping[str, str] | None = None, buffer_size: int = 8096) -> None: ...

class DecodeConfig:
    """
    Decode configuration.

    See the factory function :py:func:`~spdl.io.decode_config`.
    """

    def __init__(self, decoder: str | None = None, decoder_options: Mapping[str, str] | None = None) -> None: ...

class InternalError(AssertionError):
    pass

class AudioPackets:
    """
    Packets object containing audio samples.

    See :doc:`../io/packets_frames_concepts` for information about the Packets base concept.
    """

    def __repr__(self) -> str: ...

    def __len__(self) -> int: ...

    @property
    def timestamp(self) -> tuple[float, float] | None:
        """
        The window this packets covers, denoted by start and end time in second.

        This is the value specified by user when demuxing the stream.
        """

    @property
    def sample_rate(self) -> int:
        """The sample rate of the audio."""

    @property
    def num_channels(self) -> int:
        """The number of channels."""

    @property
    def codec(self) -> AudioCodec | None:
        """The codec."""

    def clone(self) -> AudioPackets:
        """
        Clone the packets, so that data can be decoded multiple times.

        Returns:
            A clone of the packets.
        """

class VideoPackets:
    """
    Packets object containing video frames.

    See :doc:`../io/packets_frames_concepts` for information about the Packets base concept.
    """

    def get_timestamps(self, *, raw: bool = False) -> list[float]:
        """
        Get the timestamp of packets.

        By default, the returned timestamps are sorted by display time,
        and if user specified a time window when demuxing, the timestamps
        outside of the window is discatded.

        Args:
            raw: If ``True``, the order of timestamps correspond to the
                order of packets, which is not necessarily ordered by
                display time.
                Also the user-specified window is not applied, so timestamps
                for all the packets are returned.

                This option is mainly for debugging.
        """

    @property
    def timestamp(self) -> tuple[float, float] | None:
        """
        The window this packets covers, denoted by start and end time in second.

        This is the value specified by user when demuxing the stream.
        """

    @property
    def pix_fmt(self) -> str:
        """The name of the pixel format, such as ``"yuv420p"``."""

    @property
    def width(self) -> int:
        """The width of video."""

    @property
    def height(self) -> int:
        """The height of video."""

    @property
    def frame_rate(self) -> tuple[int, int]:
        """
        The frame rate of the video in the form of ``(numerator, denominator)``.
        """

    @property
    def codec(self) -> VideoCodec | None:
        """The codec."""

    def __len__(self) -> int:
        """
        Returns the number of packets.

        .. note::

           Each packet typically contains one compressed frame, but it is not guaranteed.
        """

    def __repr__(self) -> str: ...

    def clone(self) -> VideoPackets:
        """
        Clone the packets, so that data can be decoded multiple times.

        Returns:
            A clone of the packets.
        """

class ImagePackets:
    """
    Packets object contain an image frame.

    See :doc:`../io/packets_frames_concepts` for information about the Packets base concept.
    """

    @property
    def pix_fmt(self) -> str:
        """The name of the pixel format, such as ``"yuv420p"``."""

    @property
    def width(self) -> int:
        """The width of image."""

    @property
    def height(self) -> int:
        """The height of image."""

    @property
    def codec(self) -> ImageCodec | None:
        """The codec."""

    def __repr__(self) -> str: ...

    def clone(self) -> ImagePackets:
        """
        Clone the packets, so that data can be decoded multiple times.

        Returns:
            A clone of the packets.
        """

class AudioFrames:
    """
    Audio frames.

    See :doc:`/io/packets_frames_concepts` for information about the Frames base concept.
    """

    @property
    def num_frames(self) -> int:
        """
        The number of audio frames. Same as ``__len__`` method.

        .. note::

           In SPDL,
           ``The number of samples`` == ``the number of frames`` x ``the number of channels``
        """

    @property
    def sample_rate(self) -> int:
        """The sample rate of audio."""

    @property
    def num_channels(self) -> int:
        """The number of channels."""

    @property
    def sample_fmt(self) -> str:
        """
        The name of sample format.

        Possible values are

        - ``"u8"`` for unsigned 8-bit integer.
        - ``"s16"``, ``"s32"``, ``"s64"`` for signed 16-bit, 32-bit and 64-bit integer.
        - ``"flt"``, ``"dbl"`` for 32-bit and 64-bit float.

        If the frame is planar format (separate planes for different channels), the
        name will be suffixed with ``"p"``. When converted to buffer, the buffer's shape
        will be channel-first format ``(channel, num_samples)`` instead of interweaved
        ``(num_samples, channel)``.
        """

    def __len__(self) -> int:
        """Returns the number of frames. Same as ``num_frames``."""

    def __repr__(self) -> str: ...

    def clone(self) -> AudioFrames:
        """
        Clone the frames, so that data can be converted to buffer multiple times.

        Returns:
            A clone of the frame.
        """

class VideoFrames:
    """
    Video frames.

    See :doc:`/io/packets_frames_concepts` for information about the Frames base concept.
    """

    @property
    def num_frames(self) -> int:
        """The number of video frames. Same as ``__len__`` method."""

    @property
    def num_planes(self) -> int:
        """
        The number of planes in the each frame.

        .. note::

           This corresponds to the number of color components, however
           it does not always match with the number of color channels when
           the frame is converted to buffer/array object.

           For example, if a video file is YUV format (which is one of the most
           common formats, and comprised of different plane sizes), and
           color space conversion is disabled during the decoding, then
           the resulting frames are converted to buffer as single channel frame
           where all the Y, U, V components are packed.

           SPDL by default converts the color space to RGB, so this is
           usually not an issue.
        """

    @property
    def width(self) -> int:
        """The width of video."""

    @property
    def height(self) -> int:
        """The height of video."""

    @property
    def pix_fmt(self) -> str:
        """The name of the pixel format."""

    def __len__(self) -> int:
        """Returns the number of frames. Same as ``num_frames``."""

    @overload
    def __getitem__(self, arg: slice, /) -> VideoFrames:
        """
        Slice frame by key.

        Args:
            key: If the key is int type, a single frame is returned as ``ImageFrames``.
                If the key is slice type, a new ``VideoFrames`` object pointing the
                corresponding frames are returned.

        Returns:
            The sliced frame.
        """

    @overload
    def __getitem__(self, arg: int, /) -> ImageFrames: ...

    @overload
    def __getitem__(self, arg: Sequence[int], /) -> VideoFrames: ...

    def get_timestamps(self) -> list[float]:
        """Get the timestamp of frames."""

    def get_pts(self) -> list[int]:
        """Get the PTS (Presentation Time Stamp) in timebase unit."""

    @property
    def time_base(self) -> tuple[int, int]:
        """
        Get the time base of PTS.

        The time base is expressed as ``(Numerator, denominator)``.

        PTS (in seconds) == PTS (in timebase unit) * Numerator / Denominator
        """

    def __repr__(self) -> str: ...

    def clone(self) -> VideoFrames:
        """
        Clone the frames, so that data can be converted to buffer multiple times.

        Returns:
            A clone of the frame.
        """

class ImageFrames:
    """
    Image frames.

    See :doc:`/io/packets_frames_concepts` for information about the Frames base concept.
    """

    @property
    def num_planes(self) -> int:
        """
        The number of planes in the each frame.

        See :py:class:`~spdl.io.VideoFrames` for a caveat.
        """

    @property
    def width(self) -> int:
        """The width of image."""

    @property
    def height(self) -> int:
        """The height of image."""

    @property
    def pix_fmt(self) -> str:
        """The name of the pixel format."""

    @property
    def metadata(self) -> dict[str, str]:
        """Metadata attached to the frame."""

    def __repr__(self) -> str: ...

    def clone(self) -> ImageFrames:
        """
        Clone the frames, so that data can be converted to buffer multiple times.

        Returns:
            A clone of the frame.
        """

    @property
    def pts(self) -> float:
        """
        The presentation time stamp of the image in the source video.

        This property is valid only when the ``ImageFrames`` is created from slicing
        :py:class:`~spdl.io.VideoFrames` object.
        """

class CPUStorage:
    """
    Allocate a block of CPU memory.

    See the factory function :py:func:`~spdl.io.cpu_storage`.
    """

def cpu_storage(size: int) -> CPUStorage: ...

class CPUBuffer:
    """
    Buffer implements array interface.

    To be passed to casting functions like :py:func:`~spdl.io.to_numpy`,
    :py:func:`~spdl.io.to_torch` and :py:func:`~spdl.io.to_numba`.
    """

    @property
    def __array_interface__(self) -> dict:
        """See https://numpy.org/doc/stable/reference/arrays.interface.html."""

class TracingSession:
    def init(self) -> None: ...

    def config(self, arg: str, /) -> None: ...

    def start(self, arg0: int, arg1: int, /) -> None: ...

    def stop(self) -> None: ...

def init_tracing() -> TracingSession: ...

@overload
def trace_counter(arg0: int, arg1: int, /) -> None: ...

@overload
def trace_counter(arg0: int, arg1: float, /) -> None: ...

def trace_event_begin(arg: str, /) -> None: ...

def trace_event_end() -> None: ...

def get_ffmpeg_log_level() -> int: ...

def set_ffmpeg_log_level(arg: int, /) -> None: ...

def register_avdevices() -> None: ...

def get_ffmpeg_filters() -> list[str]: ...

def get_ffmpeg_versions() -> dict[str, tuple[int, int, int]]: ...

def init_glog(arg: str, /) -> None: ...

class MultiStreamingVideoDemuxer:
    def done(self) -> bool: ...

    def next(self) -> dict[int, AudioPackets | VideoPackets | ImagePackets]: ...

class AudioCodec:
    """Codec metadata"""

    @property
    def name(self) -> str:
        """The name of the codec"""

    @property
    def sample_rate(self) -> int:
        """The sample rate of the audio stream"""

    @property
    def num_channels(self) -> int:
        """The number of channels in the audio stream"""

    @property
    def sample_fmt(self) -> str:
        """The sample format of the audio."""

    @property
    def time_base(self) -> tuple[int, int]:
        """
        The internal unit of time used for timestamp.

        The value is expressed as a fraction. ``(numerator, denominator)``.
        """

    @property
    def channel_layout(self) -> str:
        """The channel layout of the audio"""

    def __repr__(self) -> str: ...

class VideoCodec:
    """Codec metadata"""

    @property
    def name(self) -> str:
        """The name of the codec"""

    @property
    def width(self) -> int:
        """The width of the video."""

    @property
    def height(self) -> int:
        """The height of the video."""

    @property
    def pix_fmt(self) -> str:
        """The pixel format of the video."""

    @property
    def frame_rate(self) -> tuple[int, int]:
        """
        The frame rate of the video.

        The value is expressed as a fraction. ``(numerator, denominator)``.
        """

    @property
    def time_base(self) -> tuple[int, int]:
        """
        The internal unit of time used for timestamp.

        The value is expressed as a fraction. ``(numerator, denominator)``.
        """

    @property
    def sample_aspect_ratio(self) -> tuple[int, int]:
        """
        The aspect ratio of a single pixel.

        The value is expressed as a fraction. ``(width, height)``.
        """

class ImageCodec:
    """Codec metadata"""

    @property
    def name(self) -> str:
        """The name of the codec"""

    @property
    def width(self) -> int:
        """The width of the image."""

    @property
    def height(self) -> int:
        """The height of the image."""

    @property
    def pix_fmt(self) -> str:
        """The pixel format of the image."""

    @property
    def time_base(self) -> tuple[int, int]:
        """
        The internal unit of time used for timestamp.

        For image, the actual value should be irrelevant.
        This API is just for compatibility.

        The value is expressed as a fraction. ``(numerator, denominator)``.
        """

    @property
    def sample_aspect_ratio(self) -> tuple[int, int]:
        """
        The aspect ratio of a single pixel.

        The value is expressed as a fraction. ``(width, height)``.
        """

class Demuxer:
    def demux_audio(self, window: tuple[tuple[int, int], tuple[int, int]] | None = None, bsf: str | None = None) -> AudioPackets: ...

    def demux_video(self, window: tuple[tuple[int, int], tuple[int, int]] | None = None, bsf: str | None = None) -> VideoPackets: ...

    def demux_image(self, bsf: str | None = None) -> ImagePackets: ...

    def has_audio(self) -> bool: ...

    @property
    def audio_stream_index(self) -> int: ...

    @property
    def video_stream_index(self) -> int: ...

    @property
    def audio_codec(self) -> AudioCodec: ...

    @property
    def video_codec(self) -> VideoCodec: ...

    @property
    def image_codec(self) -> ImageCodec: ...

    def streaming_demux(self, indices: Set[int], *, num_packets: int, duration: float) -> MultiStreamingVideoDemuxer: ...

class AudioDecoder:
    """
    Decode stream of audio packets. See :py:class:`Decoder` for the detail.
    """

    def decode(self, packets: AudioPackets) -> AudioFrames | None:
        """Decode the given packets"""

    def flush(self) -> AudioFrames | None:
        """Flush the internally buffered frames. Use only at the end of stream"""

class VideoDecoder:
    """
    Decode stream of video packets. See :py:class:`Decoder` for the detail.
    """

    def decode(self, packets: VideoPackets) -> VideoFrames | None:
        """Decode the given packets"""

    def flush(self) -> VideoFrames | None:
        """Flush the internally buffered frames. Use only at the end of stream"""

class ImageDecoder:
    """Decode an image packet. See :py:class:`Decoder` for the detail."""

    def decode(self, packets: ImagePackets) -> ImageFrames | None:
        """Decode the given packets"""

    def flush(self) -> ImageFrames | None: ...

@overload
def decode_packets(packets: AudioPackets, *, decode_config: DecodeConfig | None = None, filter_desc: str | None = None, num_frames: int = -1) -> AudioFrames: ...

@overload
def decode_packets(packets: VideoPackets, *, decode_config: DecodeConfig | None = None, filter_desc: str | None = None, num_frames: int = -1) -> VideoFrames: ...

@overload
def decode_packets(packets: ImagePackets, *, decode_config: DecodeConfig | None = None, filter_desc: str | None = None, num_frames: int = -1) -> ImageFrames: ...

@overload
def convert_frames(frames: AudioFrames, storage: CPUStorage | None = None) -> CPUBuffer: ...

@overload
def convert_frames(frames: VideoFrames, storage: CPUStorage | None = None) -> CPUBuffer: ...

@overload
def convert_frames(frames: ImageFrames, storage: CPUStorage | None = None) -> CPUBuffer: ...

@overload
def convert_frames(frames: Sequence[AudioFrames], storage: CPUStorage | None = None) -> CPUBuffer: ...

@overload
def convert_frames(frames: Sequence[VideoFrames], storage: CPUStorage | None = None) -> CPUBuffer: ...

@overload
def convert_frames(frames: Sequence[ImageFrames], storage: CPUStorage | None = None) -> CPUBuffer: ...

def convert_array(vals: Annotated[ArrayLike, dict(dtype='int64', order='C', device='cpu')], storage: CPUStorage | None = None) -> CPUBuffer: ...

def create_reference_audio_frame(array: Annotated[ArrayLike, dict(shape=(None, None), device='cpu')], *, sample_fmt: str, sample_rate: int, pts: int) -> AudioFrames: ...

def create_reference_video_frame(array: Annotated[ArrayLike, dict(device='cpu')], *, pix_fmt: str, frame_rate: tuple[int, int], pts: int) -> VideoFrames: ...

class Muxer:
    def open(self, muxer_config: Mapping[str, str] | None = None) -> None: ...

    @overload
    def add_encode_stream(self, config: AudioEncodeConfig, encoder: str | None = None, encoder_config: Mapping[str, str] | None = None) -> AudioEncoder: ...

    @overload
    def add_encode_stream(self, config: VideoEncodeConfig, encoder: str | None = None, encoder_config: Mapping[str, str] | None = None) -> VideoEncoder: ...

    @overload
    def add_remux_stream(self, codec: AudioCodec) -> None: ...

    @overload
    def add_remux_stream(self, codec: VideoCodec) -> None: ...

    @overload
    def write(self, arg0: int, arg1: AudioPackets, /) -> None: ...

    @overload
    def write(self, arg0: int, arg1: VideoPackets, /) -> None: ...

    def flush(self) -> None: ...

    def close(self) -> None: ...

def muxer(arg0: str, *, format: str | None = None) -> Muxer: ...

class AudioEncodeConfig:
    """
    Configuration for encoding audio.

    See the factory function :py:func:`~spdl.io.audio_encode_config`.
    """

def audio_encode_config(*, num_channels: int, sample_fmt: str | None = None, sample_rate: int | None = None, bit_rate: int = -1, compression_level: int = -1, qscale: int = -1) -> AudioEncodeConfig: ...

class VideoEncodeConfig:
    """
    Configuration for encoding video.

    See the factory function :py:func:`~spdl.io.video_encode_config`.
    """

def video_encode_config(*, height: int, width: int, frame_rate: tuple[int, int] | None = None, pix_fmt: str | None = None, bit_rate: int = -1, compression_level: int = -1, qscale: int = -1, gop_size: int = -1, max_b_frames: int = -1, colorspace: str | None = None, color_primaries: str | None = None, color_trc: str | None = None) -> VideoEncodeConfig: ...

class VideoEncoder:
    """
    Video encoder.

    Returned by :py:meth:`Muxer.add_encode_stream`.
    """

    def encode(self, arg: VideoFrames, /) -> VideoPackets | None:
        """
        Encode video frames.

        Args:
            frames: Audio frames. Use :py:func:`create_reference_video_frame` to convert
                tensor/array objects into frames.

        Returns:
            Packets objects if encoder generates one.
        """

    def flush(self) -> VideoPackets | None:
        """
        Notify the encoder of the end of the stream and fetch the buffered packets.
        """

class AudioEncoder:
    """
    Audio encoder.

    Returned by :py:meth:`Muxer.add_encode_stream`.
    """

    def encode(self, arg: AudioFrames, /) -> AudioPackets | None:
        """
        Encode audio frames.

        Args:
            frames: Audio frames. Use :py:func:`create_reference_audio_frame` to convert
                tensor/array objects into frames.

        Returns:
            Packets objects if encoder generates one.
        """

    def flush(self) -> AudioPackets | None:
        """
        Notify the encoder of the end of the stream and fetch the buffered packets.
        """

    @property
    def frame_size(self) -> int:
        """
        The number of frames that the internal encoder can handle at a time.

        Some audio encoders are strict on the number of frames it can handle at a time.
        In such case, retrieve the number of expected frames (par channel) here,
        slice data accordingly, then encode slice by slice.
        """

class FilterGraph:
    """
    Construct a filter graph

    Args:
        filter_desc: A filter graph description.

    .. seealso::

       - :py:func:`get_buffer_desc`, :py:func:`get_abuffer_desc`: Helper functions
         for constructing input audio/video frames.

    .. admonition:: Example - Audio filtering (passthrough)

       For video processing use ``abuffer`` for input and ``abuffersink`` for output.

       .. code-block::

          filter_desc = "abuffer=time_base=1/44100:sample_rate=44100:sample_fmt=s16:channel_layout=1c,anull,abuffersink"
          filter_graph = FilterGraph(filter_desc)

          filter_graph.add_frames(frames)
          frames = filter_graph.get_frames()

       .. code-block::

          +------------------+
          | Parsed_abuffer_0 |default--[44100Hz s16:mono]--Parsed_anull_1:default
          |    (abuffer)     |
          +------------------+

                                                                +----------------+
          Parsed_abuffer_0:default--[44100Hz s16:mono]--default| Parsed_anull_1 |default--[44100Hz s16:mono]--Parsed_abuffersink_2:default
                                                                |    (anull)     |
                                                                +----------------+

                                                              +----------------------+
          Parsed_anull_1:default--[44100Hz s16:mono]--default| Parsed_abuffersink_2 |
                                                              |    (abuffersink)     |
                                                              +----------------------+

    .. admonition:: Example - Video filtering (passthrough)

       For video processing use ``buffer`` for input and ``buffersink`` for output.

       .. code-block::

          filter_desc = "buffer=video_size=320x240:pix_fmt=yuv420p:time_base=1/12800:pixel_aspect=1/1,null,buffersink"
          filter_graph = FilterGraph(filter_desc)

          filter_graph.add_frames(frames)
          frames = filter_graph.get_frames()

       .. code-block::

          +-----------------+
          | Parsed_buffer_0 |default--[320x240 1:1 yuv420p]--Parsed_null_1:default
          |    (buffer)     |
          +-----------------+

                                                                  +---------------+
          Parsed_buffer_0:default--[320x240 1:1 yuv420p]--default| Parsed_null_1 |default--[320x240 1:1 yuv420p]--Parsed_buffersink_2:default
                                                                  |    (null)     |
                                                                  +---------------+

                                                                +---------------------+
          Parsed_null_1:default--[320x240 1:1 yuv420p]--default| Parsed_buffersink_2 |
                                                                |    (buffersink)     |
                                                                +---------------------+

    .. admonition:: Example - Multiple Inputs

       Suffix the ``buffer``/``abuffer`` with node name so that it can be referred later.

       .. code-block::

          filter_desc = "buffer@in0=video_size=320x240:pix_fmt=yuv420p:time_base=1/12800:pixel_aspect=1/1 [in0];buffer@in1=video_size=320x240:pix_fmt=yuv420p:time_base=1/12800:pixel_aspect=1/1 [in1],[in0] [in1] vstack,buffersink"
          filter_graph = FilterGraph(filter_desc)

          filter_graph.add_frames(frames0, key="buffer@in0")
          filter_graph.add_frames(frames1, key="buffer@in1")
          frames = filter_graph.get_frames()

       .. code-block::

          +------------+
          | buffer@in0 |default--[320x240 1:1 yuv420p]--Parsed_vstack_2:input0
          |  (buffer)  |
          +------------+

          +------------+
          | buffer@in1 |default--[320x240 1:1 yuv420p]--Parsed_vstack_2:input1
          |  (buffer)  |
          +------------+

                                                           +-----------------+
          buffer@in0:default--[320x240 1:1 yuv420p]--input0| Parsed_vstack_2 |default--[320x480 1:1 yuv420p]--Parsed_buffersink_3:default
          buffer@in1:default--[320x240 1:1 yuv420p]--input1|    (vstack)     |
                                                           +-----------------+

                                                                  +---------------------+
          Parsed_vstack_2:default--[320x480 1:1 yuv420p]--default| Parsed_buffersink_3 |
                                                                  |    (buffersink)     |
                                                                  +---------------------+

    .. admonition:: Example - Multiple outputs

       Suffix the ``buffersink``/``abuffersink`` with node name so that it can be referred later.

       .. code-block::

          filter_desc = "buffer=video_size=320x240:pix_fmt=yuv420p:time_base=1/12800:pixel_aspect=1/1 [in];[in] split [out0][out1];[out0] buffersink@out0;[out1] buffersink@out1"
          filter_graph = FilterGraph(filter_desc)

          filter_graph.add_frames(frames)
          frames0 = filter_graph.get_frames(key="buffersink@out0")
          frames1 = filter_graph.get_frames(key="buffersink@out1")

       .. code-block::

          +-----------------+
          | Parsed_buffer_0 |default--[320x240 1:1 yuv420p]--Parsed_split_1:default
          |    (buffer)     |
          +-----------------+

                                                                  +----------------+
          Parsed_buffer_0:default--[320x240 1:1 yuv420p]--default| Parsed_split_1 |output0--[320x240 1:1 yuv420p]--buffersink@out0:default
                                                                  |    (split)     |output1--[320x240 1:1 yuv420p]--buffersink@out1:default
                                                                  +----------------+

                                                                 +-----------------+
          Parsed_split_1:output0--[320x240 1:1 yuv420p]--default| buffersink@out0 |
                                                                 |  (buffersink)   |
                                                                 +-----------------+

                                                                 +-----------------+
          Parsed_split_1:output1--[320x240 1:1 yuv420p]--default| buffersink@out1 |
                                                                 |  (buffersink)   |
                                                                 +-----------------+


    .. admonition:: Example - Multimedia filter

       Using `multimedia filters <https://ffmpeg.org/ffmpeg-filters.html#Multimedia-Filters>`_
       allows to convert audio stream to video stream.

       .. code-block::

          filter_desc = "abuffer=time_base=1/44100:sample_rate=44100:sample_fmt=s16:channel_layout=1c,showwaves,buffersink"
          filter_graph = FilterGraph(filter_desc)

          filter_graph.add_frames(audio_frames)
          video_frames = filter_graph.get_frames()

       .. code-block::

          +------------------+
          | Parsed_abuffer_0 |default--[44100Hz s16:mono]--Parsed_showwaves_1:default
          |    (abuffer)     |
          +------------------+

                                                                +--------------------+
          Parsed_abuffer_0:default--[44100Hz s16:mono]--default| Parsed_showwaves_1 |default--[600x240 1:1 rgba]--Parsed_buffersink_2:default
                                                                |    (showwaves)     |
                                                                +--------------------+

                                                                  +---------------------+
          Parsed_showwaves_1:default--[600x240 1:1 rgba]--default| Parsed_buffersink_2 |
                                                                  |    (buffersink)     |
                                                                  +---------------------+

    See Also:
        - :doc:`../io/advanced_filtering` - Complete guide to complex filter graphs
        - :doc:`../io/filtering` - Basic filter usage
    """

    def __init__(self, filter_desc: str) -> None: ...

    def add_frames(self, frames: AudioFrames | VideoFrames | ImageFrames, *, key: str | None = None) -> None:
        """
        Add a frame to an input node of the filter graph.

        Args:
            frames: An input frames object.
            key: The name of the input node.
                This is required when the graph has multiple input nodes.
        """

    def flush(self) -> None:
        """Notify the graph that all the input stream reached the end."""

    def get_frames(self, *, key: str | None = None) -> AudioFrames | VideoFrames | ImageFrames | None:
        """
        Get a frame from an output node of the filter graph.

        Args:
            key: The name of the output node.
                This is required when the graph has multiple output nodes.

        Returns:
            A Frames object if an output is ready, otherwise ``None``.
        """

    def dump(self) -> str: ...

def make_filter_graph(filter_desc: str) -> FilterGraph: ...

class VideoBSF:
    def filter(self, packets: VideoPackets, *, flush: bool = False) -> VideoPackets | None: ...

    def flush(self) -> VideoPackets | None: ...

class AudioBSF:
    def filter(self, packets: AudioPackets, *, flush: bool = False) -> AudioPackets | None: ...

    def flush(self) -> AudioPackets | None: ...

class ImageBSF:
    def filter(self, packets: ImagePackets, *, flush: bool = False) -> ImagePackets | None: ...

    def flush(self) -> ImagePackets | None: ...

def apply_bsf(packets: VideoPackets, bsf: str) -> VideoPackets | None: ...
