# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.
#
# This source code is licensed under the BSD-style license found in the
# LICENSE file in the root directory of this source tree.

__all__ = [
    "load_npy",
    "load_npz",
    "NpzFile",
]
import ast
import struct
from collections.abc import Iterator, Mapping
from dataclasses import dataclass
from typing import Any

import numpy as np
from numpy.lib.format import MAGIC_LEN, MAGIC_PREFIX
from numpy.typing import NDArray

# Importing `spdl.io.lib` instead of `spdl.io.lilb._archive`
# so as to delay the import of C++ extension module
from . import lib as _libspdl

# pyre-strict


@dataclass
class _ArrayInterface:
    shape: tuple[int, ...]  # pyre-ignore: [35]
    typestr: str  # pyre-ignore: [35]
    data: memoryview  # pyre-ignore: [35]
    offset: int = 0  # pyre-ignore: [35]
    version: int = 3  # pyre-ignore: [35]

    @property
    def __array_interface__(self) -> dict[str, Any]:
        return {
            "shape": self.shape,
            "typestr": self.typestr,
            "data": self.data,
            "offset": self.offset,
            "version": self.version,
        }


def _get_header_size_info(version: tuple[int, int]) -> tuple[str, str]:
    match version:
        case (1, 0):
            return ("<H", "latin1")
        case (2, 0):
            return ("<I", "latin1")
        case (3, 0):
            return ("<I", "utf8")
        case _:
            raise ValueError(f"Unexpected version {version}.")


def load_npy(
    data: bytes | bytearray | memoryview,  # pyre-ignore
    *,
    copy: bool = False,
) -> NDArray:
    """Load NumPy NDArray from bytes.

    This function loads NumPy NDArray from memory. It is equivalent to
    ``numpy.load(io.BytesIO(data))``, but it is more efficient.

    .. note::

       This function does not support ``object`` dtype, and Fortran order.

    Example:

        >>> ref = np.arange(20)
        >>> buffer = BytesIO()
        >>> np.save(buffer, ref)
        >>> buffer.seek(0)
        >>> data = buffer.getvalue()
        >>> restore = spdl.io.load_npy(data)
        >>> assert np.array_equal(restore, ref)

    Args:
        data: The data generated by :py:func:`numpy.save` function.
            (Note that it is different from :py:meth:`numpy.ndarray.tobytes`,
            which does not contain shape and dtype.)
        copy: Whether to copy data. Default: no copy.

    Returns:
        The restored array data.

    .. seealso::

       - :py:mod:`numpy.lib.format`: The detail of NPY serialization is found.

       - `_read_array_header <https://github.com/numpy/numpy/blob/v2.2.0/numpy/lib/format.py#L604>`_:
         The function called by :py:func:`numpy.load` to parse the header.
       - `_read_array <https://github.com/numpy/numpy/blob/v2.2.0/numpy/lib/format.py#L762>`_:
         The function called by :py:func:`numpy.load` when loading the data region of NPY file.

    .. note::

       There is a branch in
       `_read_array <https://github.com/numpy/numpy/blob/v2.2.0/numpy/lib/format.py#L827-L854>`_
       function where the execution can call some faster implementation.
       However, :py:class:`~io.BytesIO` does not meet the condition.
       (:py:func:`~numpy.lib.format.isfileobj` function returns ``False`` for :py:class:`io.BytesIO`.
       [`source <https://github.com/numpy/numpy/blob/v2.2.0/numpy/lib/format.py#L999>`_])
       Even if the execution takes the faster :py:func:`numpy.fromfile` path, it
       `creates a new array <https://github.com/numpy/numpy/blob/v2.2.0/numpy/_core/records.py#L935-L939>`_.

    """
    if len(data) < MAGIC_LEN:
        raise ValueError("The input data is too short.")

    view = memoryview(data)
    magic_str = view[:MAGIC_LEN].tobytes()
    if not magic_str.startswith(MAGIC_PREFIX):
        raise ValueError(rf"Expected the data to start with {MAGIC_PREFIX}.")

    major, minor = magic_str[-2:]
    hlength_type, encoding = _get_header_size_info((major, minor))

    info_length_size = struct.calcsize(hlength_type)
    info_start = MAGIC_LEN + info_length_size

    if len(data) < info_start:
        raise ValueError("Failed to parse info. The input data is invalid.")
    info_length_str = data[MAGIC_LEN:info_start]
    info_length = struct.unpack(hlength_type, info_length_str)[0]

    data_start = info_start + info_length
    if len(data) < data_start:
        raise ValueError(
            "Failed to parse data. The recorded data size exceeds the provided data size."
        )
    info_str = view[info_start:data_start].tobytes()

    info = ast.literal_eval(info_str.decode(encoding))

    if info.get("fortran_order"):
        raise ValueError(
            "Array saved with `format_order=True is not supported. Please use `numpy.load`."
        )

    # TODO: Try `numpy.frombuffer``
    # https://github.com/numpy/numpy/blob/e20317a43d3714f9085ad959f68c1ba6bc998fcd/numpy/_core/src/multiarray/ctors.c#L3711
    aif = _ArrayInterface(
        shape=info["shape"],
        typestr=info["descr"],
        data=view,
        offset=data_start,
        version=2,
    )

    return np.array(aif, copy=copy)


class NpzFile(Mapping):
    """NpzFile()
    A class mimics the behavior of :py:class:`numpy.lib.npyio.NpzFile`.

    It is a thin wrapper around a zip archive, and implements
    :py:class:`collections.abc.Mapping` interface.

    See :py:func:`load_npz` for the usage.
    """

    def __init__(self, data: bytes, meta: dict[str, tuple[int, int, int, int]]) -> None:
        self._data = memoryview(data)  # pyre-ignore
        self._meta = meta
        self.files: list[str] = [f.removesuffix(".npy") for f in meta]

    def __iter__(self) -> Iterator[str]:
        return iter(self.files)

    def __len__(self) -> int:
        return len(self.files)

    def __getitem__(self, key: str) -> NDArray:
        """Provide dictionary-like access to array data.

        One difference from the regular dictionary access is that
        it also supports accessing the item without ``.npy`` suffix
        in the key. This matches the behavior of :py:class:`numpy.lib.npyio.NpzFile`.
        """
        if key in self._meta:
            pass
        elif key in self.files:
            key = f"{key}.npy"
        else:
            raise KeyError(f"{key} is not a file in the archive")

        start, compressed_size, uncompressed_size, compression_method = self._meta[key]
        match compression_method:
            case 0:
                return load_npy(self._data[start : start + compressed_size])
            case 8:
                return load_npy(
                    _libspdl._archive.inflate(
                        self._data.obj, start, compressed_size, uncompressed_size
                    )
                )
            case _:
                raise ValueError(
                    "Compression method other than DEFLATE is not supported."
                )

    def __contains__(self, key: str) -> bool:
        return key in self._meta or key in self.files

    def __repr__(self) -> str:
        return f"NpzFile object with {len(self)} entries."


def load_npz(data: bytes) -> NpzFile:
    """**[Experimental]** Load a numpy archive file (``npz``).

    It is almost a drop-in replacement for :py:func:`numpy.load` function,
    but it only supports the basic use cases.

    This function uses the C++ implementation of the zip archive reader, which
    releases the GIL. So it is more efficient than the official NumPy implementation
    for supported cases.

    Args:
        data: The data to load.

    Example

       >>> x = np.arange(10)
       >>> y = np.sin(x)
       >>>
       >>> with tempfile.TemporaryFile() as f:
       ...     np.savez(f, x=x, y=y)
       ...     f.seek(0)
       ...     data = spdl.io.load_npz(f.read())
       ...
       >>> assert np.array_equal(data["x"], x)
       >>> assert np.array_equal(data["y"], y)

    """
    meta = {val[0]: val[1:] for val in _libspdl._archive.parse_zip(data)}
    return NpzFile(data, meta)
