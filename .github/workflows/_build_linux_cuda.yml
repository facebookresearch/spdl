name: " - Wheel (Linux / CUDA)"

on:
  workflow_call:
    inputs:
      machine:
        type: string
        default: "8-core-ubuntu-22.04"
      machine_gpu:
        type: string
        default: "4-core-ubuntu-gpu-t4"
      build_container:
        required: true
        type: string
      arch:
        type: string
        default: x86_64
      python-version:
        required: true
        type: string
      cuda-version:
        required: true
        type: string
      use-nvdec:
        default: ""
        type: string
      run-test:
        required: false
        default: 'false'
        type: string

env:
  ARTIFACT: wheel-linux-py${{ inputs.python-version }}-cuda${{ inputs.cuda-version }}${{ inputs.use-nvdec }}
  SPDL_USE_TRACING: 1
  SPDL_BUILD_STUB: 0
  SPDL_USE_CUDA: 1
  SPDL_USE_NVCODEC: "${{ inputs.use-nvdec == 'nvdec' }}"

jobs:
  build:
    runs-on: "${{ inputs.machine }}"
    container: "${{ inputs.build_container }}"
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Build
        run: |
          set -ex

          # Quick sanity check
          nvcc --version

          # We need zlib, which is usually found by default,
          # but this custom docker does not have one in locations
          # that are managed by the OS package manager.
          #
          # Conda has one so we use it.
          # $ find / -name zlib.h -type f
          # /opt/conda/include/zlib.h
          export PATH="${PATH}:/opt/conda/"
          
          wget -qO- https://astral.sh/uv/install.sh | sh
          source $HOME/.local/bin/env
          uv python list --only-installed

          py_ver="$(echo '${{ inputs.python-version }}' | sed 's|[^0-9]*||g')"
          python_exe="/opt/python/cp${py_ver}-cp${py_ver}/bin/python"
          uv venv --python "${python_exe}"
          uv build --all-packages --wheel

          ./packaging/repair_wheels.sh "manylinux_2_27_${{ inputs.arch }}" ./dist ~/package

      - uses: actions/upload-artifact@v4
        name: Upload build artifact
        with:
          name: "${{ env.ARTIFACT }}"
          path: ~/package
          if-no-files-found: error
          retention-days: 7
          overwrite: true

      - name: Check package
        run: |
          py_ver="$(echo '${{ inputs.python-version }}' | sed 's|[^0-9]*||g')"
          python_exe="/opt/python/cp${py_ver}-cp${py_ver}/bin/python"

          "${python_exe}" -m pip install -r ./packaging/requirements.txt
          "${python_exe}" -m twine check --strict ~/package/*.whl

  test-gpu:
    if: ${{ inputs.run-test == 'true' }}
    needs: ["build"]
    name: "test-cuda ffmpeg"
    strategy:
      fail-fast: false
      matrix:
        ffmpeg-version: ["4.4.2", "5.1", "6.1", "7.1" ]
    runs-on: "${{ inputs.machine_gpu }}"
    defaults:
      run:
        shell: bash -el {0}
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false

      - uses: actions/download-artifact@v4
        with:
          name: "${{ env.ARTIFACT }}"
          path: ~/package

      - uses: conda-incubator/setup-miniconda@v3
        with:
          python-version: "${{ inputs.python-version }}"
          conda-remove-defaults: "true"

      - name: Unit test
        run: |
          set -ex

          # Install SPDL
          pip install $(find "${HOME}/package" -name '*.whl' -depth -maxdepth 1)

          # Install PyTorch and others
          pip install pytest

          cu_ver="${{ inputs.cuda-version }}"
          # Remove dots
          cu_ver="${cu_ver//[.]/}"
          pip install numpy torch --index-url "https://download.pytorch.org/whl/cu${cu_ver}"

          # Install FFmpeg
          # Note: Somehow FFmepg 5.1 does not install libiconv so specifying it here
          conda install -q -c conda-forge "ffmpeg==${{ matrix.ffmpeg-version }}" libiconv
          # For some reason, the FFmpeg dynamic libraries are not found.
          export "LD_LIBRARY_PATH=${CONDA_PREFIX}/lib"

          # Run test
          python -c 'import spdl.io.utils;assert spdl.io.utils.is_cuda_available()'
          if "${{ inputs.use-nvdec == 'nvdec' }}" ; then
              python -c 'import spdl.io.utils;assert spdl.io.utils.is_nvcodec_available()'
          fi
          pytest -v \
              -k 'not nvdec' \
              tests/spdl_unittest/cuda/ \
              tests/spdl_unittest/io/

  test-cpu:
    if: "${{ inputs.run-test == 'true' }}"
    needs: ["build"]
    name: "test-cpu ffmpeg ${{ matrix.ffmpeg-version }}"
    strategy:
      fail-fast: false
      matrix:
        ffmpeg-version: ["4.4.2", "5.1", "6.1", "7.1"]
    runs-on:  ${{ inputs.machine }}
    defaults:
      run:
        shell: bash -el {0}
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false

      - uses: actions/download-artifact@v4
        with:
          name: "${{ env.ARTIFACT }}"
          path: ~/package

      - uses: conda-incubator/setup-miniconda@v3
        with:
          python-version: ${{ inputs.python-version }}
          conda-remove-defaults: "true"

      - name: Unit test
        run: |
          # Install SPDL
          pip install $(find "${HOME}/package" -name '*.whl' -depth -maxdepth 1)

          # Install PyTorch and others
          pip install numpy torch --index-url https://download.pytorch.org/whl/cpu
          conda install -q numba pytest

          # Install FFmpeg
          # Note: Somehow FFmepg 5.1 does not install libiconv so specifying it here
          conda install -q -c conda-forge "ffmpeg==${{ matrix.ffmpeg-version }}" libiconv
          # For some reason, the dynamic libraries are not found.
          export "LD_LIBRARY_PATH=${CONDA_PREFIX}/lib"

          # Run test
          pytest -v \
              tests/spdl_unittest/io/ \
              tests/spdl_unittest/dataloader/
