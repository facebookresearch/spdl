name: " - Wheel (Linux / CUDA)"

on:
  workflow_call:
    inputs:
      machine:
        type: string
        default: "8-core-ubuntu-22.04"
      machine_gpu:
        type: string
        default: "4-core-ubuntu-gpu-t4"
      build_container:
        required: true
        type: string
      arch:
        type: string
        default: x86_64
      python-version:
        required: true
        type: string
      free-threaded:
        description: 'Whether FT Python or not. Valid value is "ft" or not.'
        default: ''
        type: string
      cuda-version:
        required: true
        type: string
      use-nvdec:
        default: "nvdec"
        type: string
        description: 'Whether build NVDEC extension. Valid value is "nvdec" or not.'
      run-test:
        required: false
        default: 'false'
        type: string

env:
  ARTIFACT: wheel-linux-py${{ inputs.python-version }}${{ inputs.free-threaded }}-cuda${{ inputs.cuda-version }}${{ inputs.use-nvdec }}
  SPDL_USE_TRACING: 1
  SPDL_BUILD_STUB: 0
  SPDL_USE_CUDA: 1
  SPDL_USE_NVCODEC: "${{ inputs.use-nvdec == 'nvdec' }}"

jobs:
  build:
    runs-on: "${{ inputs.machine }}"
    container: "${{ inputs.build_container }}"
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false

      - name: Build
        run: |
          set -ex

          # Quick sanity check
          nvcc --version

          # We need zlib, which is usually found by default,
          # but this custom docker does not have one in locations
          # that are managed by the OS package manager.
          #
          # Conda has one so we use it.
          # $ find / -name zlib.h -type f
          # /opt/conda/include/zlib.h
          export PATH="${PATH}:/opt/conda/"
          
          wget -qO- https://astral.sh/uv/install.sh | sh
          source $HOME/.local/bin/env
          uv --version
          uv python list --only-installed

          # note: 3.13t is in cp313-cp313t directory, others are in cp313-cp313 directory
          v=${{ inputs.python-version }}
          dir="cp${v//[.t]/}-cp${v//[.]/}"
          if [[ "${{ inputs.free-threaded }}" == 'ft' ]]; then
            dir="${dir}t"
          fi
          python_exe="/opt/python/${dir}/bin/python"
          "${python_exe}" --version
          uv venv --python "${python_exe}"
          if [[ "${{ inputs.free-threaded }}" == 'ft' ]]; then
            source .venv/bin/activate
          fi
          uv build --no-python-downloads --all-packages --wheel

          ./packaging/repair_wheels.sh "manylinux_2_27_${{ inputs.arch }}" ./dist ~/package

      - uses: actions/upload-artifact@v4
        name: Upload build artifact
        with:
          name: "${{ env.ARTIFACT }}"
          path: ~/package
          if-no-files-found: error
          retention-days: 7
          overwrite: true

      - name: Check package
        run: |
          # CFFI is not available yet for 3.13t, which prevents the installation of twine
          # https://github.com/python-cffi/cffi/releases/tag/v1.17.0
          # So we use the regular 3.13
          v=${{ inputs.python-version }}
          dir="cp${v//[.t]/}-cp${v//[.t]/}"
          python_exe="/opt/python/${dir}/bin/python"
          "${python_exe}" -m pip install -r ./packaging/requirements.txt
          "${python_exe}" -m twine check --strict ~/package/*.whl

  test-gpu:
    if: ${{ inputs.run-test == 'true' }}
    needs: ["build"]
    name: "test-cuda ffmpeg"
    strategy:
      fail-fast: false
      matrix:
        ffmpeg-version: ["4.4.2", "5.1", "6.1", "7.1" ]
    runs-on: "${{ inputs.machine_gpu }}"
    defaults:
      run:
        shell: bash -el {0}
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false

      - uses: actions/download-artifact@v4
        with:
          name: "${{ env.ARTIFACT }}"
          path: ~/package

      - uses: conda-incubator/setup-miniconda@v3
        if: ${{ inputs.free-threaded == 'ft' }}
        with:
          python-version: ${{ inputs.python-version }}t
          conda-remove-defaults: "true"

      - uses: conda-incubator/setup-miniconda@v3
        if: ${{ inputs.free-threaded != 'ft' }}
        with:
          python-version: ${{ inputs.python-version }}
          conda-remove-defaults: "true"

      - name: Unit test
        run: |
          set -ex

          if [[ "${{ inputs.free-threaded }}" == 'ft' ]]; then
            conda install -q -c conda-forge python-freethreading
          fi

          # Install SPDL
          pip install $(find "${HOME}/package" -name '*.whl' -depth -maxdepth 1)

          # Install PyTorch and others
          pip install pytest

          cu_ver="${{ inputs.cuda-version }}"
          cu_ver="${cu_ver//[.]/}"  # Remove dots
          pip install numpy torch --index-url "https://download.pytorch.org/whl/cu${cu_ver}"

          # Install FFmpeg
          # Note: Somehow FFmepg 5.1 does not install libiconv so specifying it here
          conda install -q -c conda-forge "ffmpeg==${{ matrix.ffmpeg-version }}" libiconv

          # Sort out library paths
          # 1. For some reason, the FFmpeg dynamic libraries are not found.
          export "LD_LIBRARY_PATH=${CONDA_PREFIX}/lib"
          # 2. CUDA runtime PyPI package is installed as part of PyTorch installation.
          # However, it is installed in site-packages dir, which is not in PATH by default.
          # (PyTorch has a special way to load it, but we don't do that)
          cudart_dir="$(
            python -c 'import os.path,nvidia.cuda_runtime;print(os.path.join(nvidia.cuda_runtime.__path__[0], "lib"))'
          )"
          export "LD_LIBRARY_PATH=${LD_LIBRARY_PATH}:${cudart_dir}"

          # Run test
          python -c 'import spdl.io.utils;assert spdl.io.utils.built_with_cuda()'
          if "${{ inputs.use-nvdec == 'nvdec' }}" ; then
              python -c 'import spdl.io.utils;assert spdl.io.utils.built_with_nvcodec()'
          fi
          pytest -v \
              tests/spdl_unittest/cuda/ \
              tests/spdl_unittest/io/

  test-cpu:
    if: "${{ inputs.run-test == 'true' }}"
    needs: ["build"]
    name: "test-cpu ffmpeg ${{ matrix.ffmpeg-version }}"
    strategy:
      fail-fast: false
      matrix:
        ffmpeg-version: ["4.4.2", "5.1", "6.1", "7.1"]
    runs-on:  ${{ inputs.machine }}
    defaults:
      run:
        shell: bash -el {0}
    steps:
      - uses: actions/checkout@v4
        with:
          persist-credentials: false

      - uses: actions/download-artifact@v4
        with:
          name: "${{ env.ARTIFACT }}"
          path: ~/package

      - uses: conda-incubator/setup-miniconda@v3
        if: ${{ inputs.free-threaded == 'ft' }}
        with:
          python-version: ${{ inputs.python-version }}t
          conda-remove-defaults: "true"

      - uses: conda-incubator/setup-miniconda@v3
        if: ${{ inputs.free-threaded != 'ft' }}
        with:
          python-version: ${{ inputs.python-version }}t
          conda-remove-defaults: "true"


      - name: Unit test
        run: |
          # Install SPDL
          pip install $(find "${HOME}/package" -name '*.whl' -depth -maxdepth 1)

          # Install PyTorch and others
          pip install numpy torch --index-url https://download.pytorch.org/whl/cpu
          conda install -q numba pytest

          # Install FFmpeg
          # Note: Somehow FFmepg 5.1 does not install libiconv so specifying it here
          conda install -q -c conda-forge "ffmpeg==${{ matrix.ffmpeg-version }}" libiconv
          # For some reason, the dynamic libraries are not found.
          export "LD_LIBRARY_PATH=${CONDA_PREFIX}/lib"

          # Run test
          pytest -v \
              tests/spdl_unittest/io/ \
              tests/spdl_unittest/dataloader/
