How to build PyTorch-style DataLoader
=====================================

The :py:class:`~spdl.pipeline.Pipeline` operates on iterable/iterator,
and applies series of functions.
The ``Pipeline`` class implements ``Iterable`` protocol,
so it can be used as ``for batch in pipeline:`` structure.

However, often times ML practitioners prefer the PyTorch-style DataLoader
that they are used to.

In this section, we look at how to build a class that resembles more
like PyTorch-style DataLoader.

First, let's look at the structural difference of SPDL pipeline and
PyTorch DataLoader (and Dataset).

The following figure illustrates how data are processed in
:py:class:`torch.utils.data.DataLoader` and
:py:class:`spdl.pipeline.Pipeline`.

.. mermaid::

flowchart
    subgraph S[SPDL Pipeline]
        direction TB

        src[Source: Iterator = Sampler] 
        subgraph DS[DataSet Equivalent]
           direction TB
            p1[pipe: Mapping: int #8594; T0] -->
            p2[pipe: Callable: T0 #8594; T1] -->
            p3[pipe: Callable: T1 #8594; T2]
        end
        src --> DS --> 
        sink[Sink #40;buffer#41;]
    end

    subgraph P[PyTorch DataLoader]
        direction TB

        Sampler -->
        DataSet[DataSet: Map: int #8594; T2] -->
        pb[Prefetch Buffer]
    end

In PyTroch DataLoader, users usually define a Dataset, and put all the logic
to create data (Tensor and a composition of others) in it.
Inside of DataLoader, a sampler object such as
:py:class:`~torch.utils.data.distributed.DistributedSampler` is created and
used as a core of iteration.
From the indices generated by the sampler, the DataLoader uses DataSet to
create the data.

In SPDL, the Dataset process should be split into smaller steps, for better
concurrent execution.
We recommend to restructure the Dataset into following.

1. A mappingâ€  from integer to an ID that identifies the target data. (such as URL)
2. Split the data loading procedure into smaller functions that have different bounding factors,
   such as network/CPU/memory.

â€  We recommend to use a list instead of map for composability.

.. code-block::

   class MyDataset:

       transforms: list[Callable] = [
           download,
           decode_image,
           resize_image,
       ]
   
       def __init__(self, urls: list[str]):
           self.urls = urls

       def __len__(self) -> int:
           return len(self.urls)

       def __getitem__(self, key: int):
           data = self.urls[key]

           for transform in self.transforms:
               data = transform(data)

           return data
